import marimo

__generated_with = "0.14.10"
app = marimo.App()


@app.cell
def _():
    import marimo as mo
    import asyncio
    import json
    import logging
    import uuid
    import re
    from typing import Optional, Tuple
    import httpx
    import os
    import fcntl
    from langfuse import Langfuse
    from pathlib import Path
    return Langfuse, Path, fcntl, httpx, json, logging, mo, os, re, uuid


@app.cell
def _(Path):
    dify_url = "http://xxxxxxxxxxxxxxxxx/v1"
    langfuse_host = "http://xxxxxxxxxxxxxxxxx"
    agent_api_key = "app-kQxxxxxxxxxxxxxxxxxCmgm"
    judge_api_key = "app-d3CxxxxxxxxxxxxxxxxxpYq0w"
    lf_public_key = "pk-lf-ad6bxxxxxxxxxxxxxxxxx7608a7d43"
    lf_secret_key = "sk-lf-67xxxxxxxxxxxxxxxxx53c17d3555c"
    the_workflow_url = "http://xxxxxxxxxxxxxxxxx/v1/workflows/run"
    lf_dataset_name = "TestingEvaluationDataset"
    judge_inputs = """
    {
      "query": "User's question about an internal policy.",
      "expected_answer": "Reference answer created by a human for evaluation.",
      "expected_answer_key_facts": [],
      "provided_answer": "The AI agent's answer to the user's question.",
      "provided_answer_key_facts": []
    }
    """.strip()
    judge_objective = """
    Assess whether the provided answer correctly matches the key facts — such as procedures, dates, functionalities, steps, etc. — outlined in the expected answer key. Focus only on factual content. Do not consider style, explanations, or non-essential context.
    """.strip()
    judge_guidelines = """
    Evaluation Steps:

    1. Compare Key Facts:
       - Compare `provided_answer_key_facts` to `expected_answer_key_facts` to determine:
         • Which key facts are matched (present and accurate)
         • Which key facts are missing
         • Which key facts are contradicted (present but inaccurate/conflicting)
       - Note any extra, non-contradictory facts in `provided_answer_key_facts` (additional details that do not conflict with expected facts).

    2. Determine Judgment:
       - Assign one of the following labels based on factual alignment:
         • "correct": All key facts from expected_answer_key_facts are included. Extra, non-contradictory information does not affect this label.
         • "partially_correct": Some key facts are included, but others are omitted; no contradictions.
         • "incorrect": Facts are inaccurate, incomplete, or contradictory.
         • "not_understood": No facts provided, vague/unhelpful/uncertain/requests clarification. Do not compare to expected facts in this case.

    3. Provide Explanation:
       - Clearly explain your reasoning for the judgment.
       - Indicate which expected key facts are matched, missing, or contradicted.
       - List extra facts in the provided answer.
       - Base your assessment only on present factual content; do not infer unstated information.

    Guidelines:
    - Always justify your judgment clearly.
    - Avoid guesses or assumptions. Focus on factual accuracy.
    - Ignore stylistic and non-essential differences.
    """.strip()
    shot_examples = """
    [
      {
        "query": "What is the procedure for requesting annual leave?",
        "expected_answer": "Employees can request annual leave through the HR Portal. The request is sent to their manager for approval. Once approved, HR will update the leave record.",
        "expected_answer_key_facts": [
          "Request yearly leave via the HR Portal",
          "Manager approval is required",
          "HR updates the record after approval"
        ],
        "provided_answer": "To request annual leave, go to the HR Portal and submit your request. Your manager must approve it. HR will record the leave after it's approved.",
        "provided_answer_key_facts": [
          "Request is made using HR Portal",
          "Manager approves request",
          "HR updates leave record"
        ],
        "label": "correct",
        "reasoning": "The provided answer matches all key facts in the expected answer. The procedure and steps are correct."
      },
      {
        "query": "How do I reset my login password?",
        "expected_answer": "Contact IT Support to reset your password. You must provide your employee ID for verification.",
        "expected_answer_key_facts": [
          "Reset by contacting IT Support",
          "Employee ID is required for verification"
        ],
        "provided_answer": "You can reset your password by emailing IT Support. They will ask for your employee number before proceeding.",
        "provided_answer_key_facts": [
          "Reset by emailing IT Support",
          "Employee number is needed"
        ],
        "label": "correct",
        "reasoning": "Both required key facts are present: the reset must happen via IT Support and verification is by employee ID (number and ID are equivalent here)."
      }
    ]
    """.strip()
    PROJECT_ROOT = Path("")
    CHECKPOINTS_DIR = PROJECT_ROOT / "checkpoints"
    PROFILES_DIR = PROJECT_ROOT / "profiles"
    PROFILE_PREFIX = "profile_"
    return CHECKPOINTS_DIR, PROFILES_DIR, PROFILE_PREFIX


@app.cell
def _(
    CHECKPOINTS_DIR,
    Langfuse,
    Path,
    fcntl,
    httpx,
    json,
    logging,
    mo,
    os,
    re,
    uuid,
):
    logger = logging.getLogger("marimo_eval")
    logger.setLevel(logging.DEBUG)   # Your logs

    formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    file_handler = logging.FileHandler("evaluation.log", mode="a", encoding="utf-8")
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # --- Evaluation class ---
    class Evaluation:
        def __init__(
            self,
            experiment_name,
            langfuse_public_key,
            langfuse_secret_key,
            langfuse_host,
            dataset_name,
            dify_agent_key,
            dify_judge_key,
            dify_base_url,
            dify_workflow_url,
            instructions_inputs,
            instructions_objective,
            instructions_guidelines,
            instructions_fewshot,
        ):
            self.langfuse_public_key = langfuse_public_key
            self.langfuse_secret_key = langfuse_secret_key
            self.langfuse_host = langfuse_host
            self.base_experiment_name = experiment_name or "marimo_eval"
            self.dataset_name = dataset_name
            self.dify_agent_key = dify_agent_key
            self.dify_base_url = dify_base_url
            self.dify_judge_key = dify_judge_key
            self.dify_workflow_url = dify_workflow_url
            self.instructions_inputs = instructions_inputs
            self.instructions_objective = instructions_objective
            self.instructions_guidelines = instructions_guidelines
            self.instructions_fewshot = instructions_fewshot
            self.langfuse = Langfuse(
                public_key=self.langfuse_public_key,
                secret_key=self.langfuse_secret_key,
                host=self.langfuse_host,
            )

        def get_dataset_items(self):
            dataset = self.langfuse.get_dataset(self.dataset_name)
            return list(dataset.items)

        # ------------ DIFY API CALLS ------------

        async def call_agent(self, client, query, conversation_id=None):
            """
            Calls the agent (HR) async, returning (answer, conversation_id, message_id)
            """
            url = f"{self.dify_base_url}/chat-messages"
            payload = {
                "inputs": {},
                "query": query,
                "response_mode": "streaming",
                "conversation_id": conversation_id or "",
                "user": "eval_marimo_demo",
            }
            headers = {
                "Authorization": f"Bearer {self.dify_agent_key}",
                "Content-Type": "application/json",
            }
            resp = await client.post(
                url, headers=headers, json=payload, timeout=120
            )
            # streaming response as text
            text = resp.text
            last_json = None
            message_id = None
            if isinstance(text, str) and text.startswith("data:"):
                # Dify returns a stream, each event as data:<json>
                # We take last workflow_finished
                matches = re.findall(r"data:\s*(\{.*?\})\n\n", text, re.DOTALL)
                for m in matches[::-1]:
                    try:
                        js = json.loads(m)
                        if js.get("event") == "workflow_finished":
                            last_json = js
                            message_id = js.get("message_id")
                            break
                    except Exception:
                        continue
            if last_json is None and resp.headers.get(
                "Content-Type", ""
            ).startswith("application/json"):
                js = resp.json()
                last_json = js
            if last_json is not None:
                answer = (
                    last_json.get("data", {})
                    .get("outputs", {})
                    .get("answer", None)
                )
                conv_id = last_json.get("conversation_id") or ""
                msg_id = last_json.get("message_id") or message_id
                return answer, conv_id, msg_id
            return None, conversation_id, None

        async def call_judge(
            self,
            client,
            query,
            expected_answer,
            provided_answer,
        ):
            """
            Calls the judge API, returns (score, category, reasoning)
            """
            payload = {
                "inputs": {
                    "query": query,
                    "expected_answer": expected_answer,
                    "actual_answer": provided_answer,
                    "inputs": self.instructions_inputs,
                    "objective": self.instructions_objective,
                    "judgment_guidelines": self.instructions_guidelines,
                    "few_shot_examples": self.instructions_fewshot,
                },
                "response_mode": "blocking",
                "conversation_id": "",
                "user": "eval_marimo_demo",
            }
            headers = {
                "Authorization": f"Bearer {self.dify_judge_key}",
                "Content-Type": "application/json",
            }
            resp = await client.post(
                self.dify_workflow_url, headers=headers, json=payload, timeout=120
            )
            # judge returns json
            try:
                data = resp.json().get("data", {})
                outputs = data.get("outputs", {})
                result = outputs.get("result", {})
                score = result.get("score")
                category = result.get("category")
                reasoning = result.get("reasoning", "")
                return score, category, reasoning
            except Exception as e:
                print(f"Judge parse error: {e}")
            return None, None, ""

        def get_checkpoint_path(self, dataset_name, experiment_name):
            CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
            dataset_part = dataset_name.strip().lower().replace(" ", "_")
            experiment_part = experiment_name.strip().lower().replace(" ", "_")
            return str(CHECKPOINTS_DIR / f"{dataset_part}_{experiment_part}.json")

        def load_completed(self, checkpoint_path):
            if os.path.exists(checkpoint_path):
                with open(checkpoint_path, "r") as f:
                    return set(json.load(f))
            return set()

        def save_completed(self, checkpoint_path, completed_ids):
            Path(checkpoint_path).parent.mkdir(parents=True, exist_ok=True)
            with open(checkpoint_path, "w") as f:
                json.dump(list(completed_ids), f)

        async def process(self):
            experiment_name = self.base_experiment_name.strip().lower().replace(" ", "_")
            dataset_name = self.dataset_name.strip().lower().replace(" ", "_")

            checkpoint_path = self.get_checkpoint_path(self.dataset_name, experiment_name)   # Note: pass both!
            lock_path = checkpoint_path + ".lock"
            lock_file = open(lock_path, "w")
            acquired_lock = False

            try:
                try:
                    fcntl.flock(lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)
                    acquired_lock = True
                except BlockingIOError:
                    # UI Feedback on lock
                    error_msg = (
                        f"❗️ <b>An evaluation run for experiment <code>{experiment_name}</code> is already in progress.</b><br>"
                        f"Please wait, or delete the lock file:<br><pre>{lock_path}</pre>"
                    )
                    mo.output.replace(mo.md(error_msg))
                    print(f"Lockfile {lock_path} is already held - Please wait.")
                    return 0

                items = self.get_dataset_items()
                total_items = len(items)

                # --- Establish experiment name with timestamp ONCE, for checkpoint/tracking ---
                experiment_name = (
                    self.base_experiment_name.strip().lower().replace(" ", "_")
                )

                # --- Checkpoint logic: Set up checkpoint path, and load previously completed items ---
                checkpoint_path = self.get_checkpoint_path(self.dataset_name, experiment_name)
                print(f"Checkpoint file will be at: {checkpoint_path}")
                completed_ids = self.load_completed(checkpoint_path)  # set of ids

                # Sort items for consistent processing order
                def item_sort(item):
                    md = getattr(item, "metadata", {}) or {}
                    return (md.get("conversation_idx", 0), md.get("turn_idx", 0))

                items.sort(key=item_sort)

                # Initialize conversation_ids dictionary
                conversation_ids = {}

                # Existing completed_ids set
                # Find the first index where item ID is not in completed_ids
                start_idx = 0
                for idx, item in enumerate(items):
                    item_id = getattr(item, "id", None) or f"idx_{idx}"
                    if item_id not in completed_ids:
                        start_idx = idx
                        break
                else:
                    # All items are completed
                    start_idx = len(items)

                # Now, process only remaining items starting from start_idx
                remaining_items = items[start_idx:]
                remaining_start_idx = start_idx  # keep track of original index
                total_remaining = len(remaining_items)

                print(
                    f"DEBUG: Items: {len(items)}, total_remaining: {total_remaining}, completed: {len(completed_ids)}"
                )
                if total_remaining == 0:
                    mo.md(
                        "✅ Nothing to process: all items have already been evaluated or no dataset items found."
                    )
                    print("Nothing to process. Exiting.")
                    return 0

                with mo.status.progress_bar(
                    range(total_remaining),
                    title="Evaluation in Progress",
                    subtitle=f"Processing dataset items (Experiment: {experiment_name})",
                    show_eta=True,
                    show_rate=True,
                ) as bar:
                    async with httpx.AsyncClient() as client:
                        processed = 0
                        skipped = 0

                        for offset, item in enumerate(remaining_items):
                            idx = start_idx + offset
                            item_id = getattr(item, "id", None) or f"idx_{idx}"
                            if item_id in completed_ids:
                                skipped += 1
                                continue  # Skip already processed items

                            # Proceed with processing
                            md = getattr(item, "metadata", {}) or {}
                            conv_idx = md.get("conversation_idx", 0)
                            turn_idx = md.get("turn_idx", 0)
                            multiturn = md.get("multiturn", False)
                            query = item.input.get("text", "") or ""
                            expected_answer = (
                                item.expected_output.get("text", "") or ""
                            )
                            conversation_id = None
                            if multiturn:
                                conversation_id = conversation_ids.get(conv_idx)
                            print(f"→ [{idx + 1}/{total_items}] Q: {query}")

                            (
                                answer,
                                new_conversation_id,
                                message_id,
                            ) = await self.call_agent(client, query, conversation_id)
                            print(
                                f"Agent returned '{answer}'  (message_id: {message_id})"
                            )
                            if new_conversation_id:
                                conversation_ids[conv_idx] = new_conversation_id

                            if not answer:
                                print(
                                    "Agent did not return an answer. Skipping judging."
                                )
                                # Mark as processed anyway to avoid infinite retry? Optional:
                                # completed_ids.add(item_id)
                                # self.save_completed(checkpoint_path, completed_ids)
                                continue

                            score, category, reasoning = await self.call_judge(
                                client, query, expected_answer, answer
                            )
                            print(
                                f"Judge result: Score={score},Category={category},Reasoning={reasoning}"
                            )

                            trace = self.langfuse.trace(
                                id=message_id or str(uuid.uuid4()),
                                input=query,
                                output=answer,
                                user_id="marimo_demo_run",
                                tags=[category or ""],
                            )

                            # link trace to dataset item
                            item.link(
                                trace,
                                experiment_name,
                                run_description="",
                                run_metadata={"category": category, "score": score},
                            )

                            try:
                                if score is not None:
                                    trace.score(
                                        name="llm_judge",
                                        value=score,
                                        comment=reasoning or "",
                                        data_type="NUMERIC",
                                    )
                                if category:
                                    trace.score(
                                        name="evaluation",
                                        value=category,
                                        comment=reasoning or "",
                                        data_type="CATEGORICAL",
                                    )
                                print(f"Logged to Langfuse for item '{item_id}'")
                            except Exception as e:
                                print(f"Score logging error: {e}")

                            # --- UPDATE CHECKPOINT! ---
                            completed_ids.add(item_id)
                            self.save_completed(checkpoint_path, completed_ids)
                            processed += 1
                            print(f"   ✔︎ Done: Cat={category}, Score={score}")
                            bar.update(1)
                mo.status.spinner("Evaluation complete!")
                print(
                    f"Evaluation done ({processed} processed, {skipped} skipped)."
                )

            finally:
                if acquired_lock:
                    try:
                        fcntl.flock(lock_file, fcntl.LOCK_UN)
                    except Exception:
                        pass
                lock_file.close()
    return Evaluation, logger


@app.cell
def _(PROFILES_DIR, PROFILE_PREFIX, json):
    def get_profiles():
        PROFILES_DIR.mkdir(parents=True, exist_ok=True)
        profiles = sorted(
            [
                f.name[len(PROFILE_PREFIX) : -5]
                for f in PROFILES_DIR.glob(f"{PROFILE_PREFIX}*.json")
            ]
        )
        print(profiles)
        return profiles


    def profile_path(name):
        return PROFILES_DIR / f"profile_{name}.json"


    def load_profile(name):
        path = profile_path(name)
        if not path.exists():
            raise FileNotFoundError(f"Profile '{name}' not found!")
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)


    def save_profile(name, data):
        PROFILES_DIR.mkdir(parents=True, exist_ok=True)
        with open(profile_path(name), "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)

    def delete_profile(name):
        path = profile_path(name)
        if not path.exists():
            raise FileNotFoundError(f"Profile '{name}' not found!")
        path.unlink()

    available_profiles = get_profiles()
    return available_profiles, delete_profile, load_profile, save_profile


@app.cell
def _(
    available_profiles,
    get_agent_key,
    get_dataset_name,
    get_dify_url,
    get_experiment_name,
    get_judge_guidelines,
    get_judge_inputs,
    get_judge_key,
    get_judge_objective,
    get_langfuse_host,
    get_lf_public_key,
    get_lf_secret_key,
    get_profile_name,
    get_shot_examples,
    mo,
    set_agent_key,
    set_dataset_name,
    set_dify_url,
    set_experiment_name,
    set_judge_guidelines,
    set_judge_inputs,
    set_judge_key,
    set_judge_objective,
    set_langfuse_host,
    set_lf_public_key,
    set_lf_secret_key,
    set_profile_name,
    set_shot_examples,
):
    marimo_experiment_name = mo.ui.text(
        label="Experiment Name",
        value=get_experiment_name(),
        full_width=True,
        on_change=set_experiment_name
    )

    marimo_dify_base_url = mo.ui.text(
        label="Base URL", value=get_dify_url(), full_width=True, on_change=set_dify_url
    )
    marimo_dify_agent_key = mo.ui.text(
        label="Agent API Key", value=get_agent_key(), full_width=True, on_change=set_agent_key
    )
    marimo_dify_judge_key = mo.ui.text(
        label="LLM-as-Judge API Key", value=get_judge_key(), full_width=True, on_change=set_judge_key
    )
    marimo_langfuse_host = mo.ui.text(
        label="Host", value=get_langfuse_host(), full_width=True, on_change=set_langfuse_host
    )
    marimo_lf_public_key = mo.ui.text(
        label="Public Key",
        value=get_lf_public_key(),
        full_width=True,
        on_change=set_lf_public_key,
    )
    marimo_lf_secret_key = mo.ui.text(
        label="Secret Key", value=get_lf_secret_key(), full_width=True, on_change=set_lf_secret_key
    )
    marimo_dataset_name = mo.ui.text(
        label="Dataset Name", value=get_dataset_name(), full_width=True, on_change=set_dataset_name
    )
    # Repeat for all Judge fields...
    marimo_judge_inputs = mo.ui.text_area(
        label="Judge inputs", rows=3, full_width=True, value=get_judge_inputs(), on_change=set_judge_inputs
    )
    marimo_judge_objective = mo.ui.text_area(
        label="Judge objective", rows=3, full_width=True, value=get_judge_objective(), on_change=set_judge_objective
    )
    marimo_judge_guidelines = mo.ui.text_area(
        label="Judge guidelines", rows=3, full_width=True, value=get_judge_guidelines(), on_change=set_judge_guidelines
    )
    marimo_shot_examples = mo.ui.text_area(
        label="Judge few-shot examples", rows=3, full_width=True, value=get_shot_examples(), on_change=set_shot_examples
    )
    marimo_run_button = mo.ui.run_button(label="Run Evaluation")

    marimo_profile_selector = mo.ui.dropdown(
        label="Choose a Profile (or blank for manual entry)",
        options=[""] + available_profiles,
        value="",
        full_width=True,
    )

    marimo_profile_save_name = mo.ui.text(
        label="Profile Name (for storing the current fields below)",
        value=get_profile_name(),
        placeholder="e.g. Agent1, Project5, etc.",
        full_width=True,
        on_change=set_profile_name
    )

    marimo_profile_save_btn = mo.ui.run_button(label="Save Profile")

    load_button = mo.ui.run_button(label="Load Profile")

    marimo_profile_delete_btn = mo.ui.run_button(label="Delete Profile", kind="danger")
    return (
        load_button,
        marimo_dataset_name,
        marimo_dify_agent_key,
        marimo_dify_base_url,
        marimo_dify_judge_key,
        marimo_experiment_name,
        marimo_judge_guidelines,
        marimo_judge_inputs,
        marimo_judge_objective,
        marimo_langfuse_host,
        marimo_lf_public_key,
        marimo_lf_secret_key,
        marimo_profile_delete_btn,
        marimo_profile_save_btn,
        marimo_profile_save_name,
        marimo_profile_selector,
        marimo_run_button,
        marimo_shot_examples,
    )


@app.cell
def _(mo):
    # Cell: State definitions
    get_lf_public_key, set_lf_public_key = mo.state("")
    get_lf_secret_key, set_lf_secret_key = mo.state("")
    get_langfuse_host, set_langfuse_host = mo.state("")
    get_dataset_name, set_dataset_name = mo.state("")
    get_agent_key, set_agent_key = mo.state("")
    get_dify_url, set_dify_url = mo.state("")
    get_judge_key, set_judge_key = mo.state("")
    get_judge_inputs, set_judge_inputs = mo.state("")
    get_judge_objective, set_judge_objective = mo.state("")
    get_judge_guidelines, set_judge_guidelines = mo.state("")
    get_shot_examples, set_shot_examples = mo.state("")
    get_experiment_name, set_experiment_name = mo.state("")
    get_profile_name, set_profile_name = mo.state("")
    return (
        get_agent_key,
        get_dataset_name,
        get_dify_url,
        get_experiment_name,
        get_judge_guidelines,
        get_judge_inputs,
        get_judge_key,
        get_judge_objective,
        get_langfuse_host,
        get_lf_public_key,
        get_lf_secret_key,
        get_profile_name,
        get_shot_examples,
        set_agent_key,
        set_dataset_name,
        set_dify_url,
        set_experiment_name,
        set_judge_guidelines,
        set_judge_inputs,
        set_judge_key,
        set_judge_objective,
        set_langfuse_host,
        set_lf_public_key,
        set_lf_secret_key,
        set_profile_name,
        set_shot_examples,
    )


@app.cell
def _(
    PROFILES_DIR,
    Path,
    delete_profile,
    load_button,
    load_profile,
    marimo_dataset_name,
    marimo_dify_agent_key,
    marimo_dify_base_url,
    marimo_dify_judge_key,
    marimo_experiment_name,
    marimo_judge_guidelines,
    marimo_judge_inputs,
    marimo_judge_objective,
    marimo_langfuse_host,
    marimo_lf_public_key,
    marimo_lf_secret_key,
    marimo_profile_delete_btn,
    marimo_profile_save_btn,
    marimo_profile_save_name,
    marimo_profile_selector,
    marimo_shot_examples,
    mo,
    os,
    save_profile,
    set_agent_key,
    set_dataset_name,
    set_dify_url,
    set_experiment_name,
    set_judge_guidelines,
    set_judge_inputs,
    set_judge_key,
    set_judge_objective,
    set_langfuse_host,
    set_lf_public_key,
    set_lf_secret_key,
    set_profile_name,
    set_shot_examples,
):
    if marimo_profile_save_btn.value:
        print("Save button clicked")
        mo.output.replace(mo.md("👀 Processing save..."))
        name = marimo_profile_save_name.value.strip()
        if not name:
            msg = "❌ Please enter a profile name before saving."
            print(msg)
            mo.output.replace(mo.md(msg))
        else:
            data = {
                "dify_url": marimo_dify_base_url.value,
                "agent_api_key": marimo_dify_agent_key.value,
                "judge_api_key": marimo_dify_judge_key.value,
                "langfuse_host": marimo_langfuse_host.value,
                "lf_public_key": marimo_lf_public_key.value,
                "lf_secret_key": marimo_lf_secret_key.value,
                "lf_dataset_name": marimo_dataset_name.value,
                "experiment_name": marimo_experiment_name.value,
                "judge_inputs": marimo_judge_inputs.value,
                "judge_objective": marimo_judge_objective.value,
                "judge_guidelines": marimo_judge_guidelines.value,
                "shot_examples": marimo_shot_examples.value,
            }
            try:
                save_profile(name, data)
                path = Path(PROFILES_DIR)
                print(f"Attempted save to {path}")
                if os.path.exists(path):
                    stat = os.stat(path)
                    bytes_written = stat.st_size
                    print(f"File '{path}' written successfully, size {bytes_written} bytes.")
                    mo.output.replace(
                        mo.md(f"✅ Profile '{name}' saved to {path} ({bytes_written} bytes). Reload notebook to see in selector.")
                    )
                else:
                    msg = f"❌ Save failed, file not found: {path}"
                    print(msg)
                    mo.output.replace(mo.md(msg))
            except Exception as e:
                print(f"Error during save: {e}", exc_info=True)
                mo.output.replace(
                    mo.md(f"❌ Error saving profile '{name}':<pre>{e}</pre>")
                )

    if marimo_profile_delete_btn.value:
        name = marimo_profile_save_name.value.strip()
        if not name:
            msg = "❌ Please enter a profile name before deleting."
            print(msg)
            mo.output.replace(mo.md(msg))
        else:
            try:
                delete_profile(name)
                print(f"Deleted profile '{name}'")
                mo.output.replace(mo.md(f"🗑️ Deleted profile '{name}'. Please reload notebook to refresh profile list."))
                set_profile_name("")  # Optionally clear text field
            except Exception as e:
                msg = f"❌ Error deleting profile '{name}':<pre>{e}</pre>"
                print(msg)
                mo.output.replace(mo.md(msg))

    # 3. When loading values, use the state setters
    if load_button.value:
        loaded_profile = marimo_profile_selector.value
        retrieved_values = load_profile(loaded_profile)
        set_lf_public_key(retrieved_values['lf_public_key'])
        set_lf_secret_key(retrieved_values['lf_secret_key'])
        set_langfuse_host(retrieved_values['langfuse_host'])
        set_dataset_name(retrieved_values['lf_dataset_name'])
        set_agent_key(retrieved_values['agent_api_key'])
        set_dify_url(retrieved_values['dify_url'])
        set_judge_key(retrieved_values['judge_api_key'])
        set_judge_inputs(retrieved_values['judge_inputs'])
        set_judge_objective(retrieved_values['judge_objective'])
        set_judge_guidelines(retrieved_values['judge_guidelines'])
        set_shot_examples(retrieved_values['shot_examples'])
        set_experiment_name(retrieved_values['experiment_name'])
        set_profile_name(loaded_profile)
    return


@app.cell
def _(
    load_button,
    marimo_dataset_name,
    marimo_dify_agent_key,
    marimo_dify_base_url,
    marimo_dify_judge_key,
    marimo_experiment_name,
    marimo_judge_guidelines,
    marimo_judge_inputs,
    marimo_judge_objective,
    marimo_langfuse_host,
    marimo_lf_public_key,
    marimo_lf_secret_key,
    marimo_profile_delete_btn,
    marimo_profile_save_btn,
    marimo_profile_save_name,
    marimo_profile_selector,
    marimo_run_button,
    marimo_shot_examples,
    mo,
):
    ui_layout = mo.vstack([
            mo.vstack([
                mo.md("### Profile Management"),
                mo.hstack([
                    marimo_profile_selector,
                    load_button
                ]),
                mo.hstack([
                    marimo_profile_save_name,
                    marimo_profile_save_btn,
                    marimo_profile_delete_btn,
                ]),
                mo.md("*Choose a profile to autofill settings. To save a new configuration, fill below, enter a name, and click save.*"),
                mo.center(mo.md("### Dify Settings")),
                mo.center(
                    marimo_dify_base_url.style({"width": "90%"})
                ),
                mo.center(
                    mo.hstack([
                        marimo_dify_agent_key.style({"width": "45%"}),
                        marimo_dify_judge_key.style({"width": "45%"}),
                    ]).style({"width": "90%"})
                ),
                mo.center(mo.md("### Langfuse Settings")),
                mo.center(
                    mo.hstack([
                        marimo_langfuse_host.style({"width": "30%"}),
                        marimo_lf_public_key.style({"width": "30%"}),
                        marimo_lf_secret_key.style({"width": "30%"}),
                    ]).style({"width": "90%"})
                ),
                mo.center(mo.md("### Dataset")),
                mo.center(marimo_dataset_name.style({"width": "90%"})),
                mo.center(marimo_experiment_name.style({"width": "90%"})),
                mo.center(mo.md("### Judge Configuration")),
                mo.center(
                    mo.vstack([
                        marimo_judge_inputs,
                        marimo_judge_objective,
                        marimo_judge_guidelines,
                        marimo_shot_examples
                    ]).style({"width": "90%"})
                ),
                mo.center(marimo_run_button),
                mo.md("*Note: Experiment names are used for run tracking and checkpointing. "
                      "If you reuse an experiment name that completed for this dataset, you'll be notified to choose a new name. "
                      "If previous run was incomplete, it will resume from where it left off.*"),
            ])
        ])

    ui_layout
    return


@app.cell
async def _(
    CHECKPOINTS_DIR,
    Evaluation,
    Langfuse,
    httpx,
    json,
    logger,
    marimo_dataset_name,
    marimo_dify_agent_key,
    marimo_dify_base_url,
    marimo_dify_judge_key,
    marimo_experiment_name,
    marimo_judge_guidelines,
    marimo_judge_inputs,
    marimo_judge_objective,
    marimo_langfuse_host,
    marimo_lf_public_key,
    marimo_lf_secret_key,
    marimo_run_button,
    marimo_shot_examples,
    mo,
):
    mo.stop(not marimo_run_button.value, "Click the run button to start evaluation.")

    def check_dify_agent_connectivity(base_url, agent_key, logger):
        """
        Returns (ok, reason) tuple for Dify Agent API connectivity
        """
        chat_url = f"{base_url.rstrip('/')}/chat-messages"
        print(f"check_dify_agent_connectivity: base_url={base_url}, chat_url={chat_url}")
        headers = {
            "Authorization": f"Bearer {agent_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "inputs": {},
            "query": "connectiontest",
            "response_mode": "streaming",
            "conversation_id": "",
            "user": "eval_marimo_demo",
        }
        print(f"Payload for Dify agent connectivity: {payload}")
        try:
            with httpx.Client(timeout=120) as client:
                resp = client.post(chat_url, headers=headers, json=payload)
                print(f"Response status: {resp.status_code}")
                print(f"Response content: {resp.text}")
                if resp.status_code in (401, 403):
                    return False, "Invalid Dify Agent API Key"
                if resp.status_code == 404:
                    return False, f"/chat-messages endpoint not found. (Check base URL)"
                if not (resp.status_code // 100 == 2):
                    return False, f"/chat-messages returned HTTP {resp.status_code}."
        except Exception as e:
            print(f"Failed to connect to Dify agent endpoint at {chat_url}: {e}")
            return False, f"Could not connect to Dify agent endpoint: {e}"
        return True, None

    def check_dify_judge_connectivity(base_url, judge_key, logger):
        """
        Returns (ok, reason) tuple for Dify Judge API connectivity
        """
        judge_url = f"{base_url.rstrip('/')}/workflows/run"
        print(f"check_dify_judge_connectivity: base_url={base_url}, judge_url={judge_url}")
        headers = {
            "Authorization": f"Bearer {judge_key}",
            "Content-Type": "application/json",
        }
        payload = {
            "inputs": {
                "query": "__connection_test__",
                "expected_answer": "connection_test",
                "actual_answer": "connection_test",
                "inputs": "these inputs",
                "objective": "an objective",
                "judgment_guidelines": "some guidelines",
                "few_shot_examples": "some examples",
            },
            "response_mode": "streaming",
            "conversation_id": "",
            "user": "conn_test"
        }
        print(f"Payload for Dify Judge connectivity: {payload}")
        try:
            with httpx.Client(timeout=10) as client:
                resp = client.post(judge_url, headers=headers, json=payload)
                print(f"Response status: {resp.status_code}")
                print(f"Response content: {resp.text}")
                if resp.status_code in (401, 403):
                    return False, "Invalid Dify Judge API Key"
                if resp.status_code == 404:
                    return False, f"/workflows/run endpoint not found. (Check base URL)"
                if not (resp.status_code // 100 == 2):
                    print(f"Dify Judge 400 response: {resp.text}")
                    return False, f"/workflows/run returned HTTP {resp.status_code}."
        except Exception as e:
            print(f"Failed to connect to Dify judge endpoint at {judge_url}: {e}")
            return False, f"Could not connect to Dify judge endpoint: {e}"
        return True, None

    def check_langfuse_connectivity(public_key, secret_key, host, logger):
        try:
            print(f"Checking Langfuse connectivity with host={host}")
            print(f"Langfuse pk: {public_key} and sk: {secret_key}")
            lf = Langfuse(
                public_key=public_key,
                secret_key=secret_key,
                host=host
            )
            print(f"Created Langfuse instance: {lf}")
            auth_check_result = lf.auth_check()
            print(f"Langfuse auth_check result: {auth_check_result}")
            if auth_check_result:
                return True, None
            else:
                return False, "Invalid Langfuse public/secret keys"
        except Exception as e:
            print(f"Langfuse connection error: {e}")
            msg = str(e)
            print(f"Langfuse exception message: {msg}")
            if (
                "401" in msg
                or "403" in msg
                or "invalid" in msg.lower()
                or "Unauthorized" in msg
            ):
                return False, "Invalid Langfuse public/secret keys"
            if "not found" in msg.lower():
                return False, "Langfuse host not found"
            return False, f"Langfuse error: {msg}"

    def check_dataset_exists(lf, dataset_name, logger):
        try:
            print(f"Attempting to get dataset: {dataset_name}")
            ds = lf.get_dataset(dataset_name)
            print(f"Dataset fetched: {ds}")
            return ds is not None
        except Exception as e:
            print(f"Error checking dataset: {e}")
            return False

    def get_checkpoint(dataset_name, experiment_name):
        CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)
        dataset_part = dataset_name.strip().lower().replace(" ", "_")
        experiment_part = experiment_name.strip().lower().replace(" ", "_")
        return CHECKPOINTS_DIR / f"{dataset_part}_{experiment_part}.json"

    def existing_experiment_checkpoint(dataset_name, experiment_name, logger):
        ckp_path = get_checkpoint(dataset_name, experiment_name)
        print(f"Checkpoint path: {ckp_path}")
        if ckp_path.exists():
            ids = []
            try:
                print("Checkpoint file exists, attempting to load")
                with open(ckp_path, "r") as f:
                    ids = json.load(f)
                print(f"Loaded checkpoint IDs: {ids}")
            except Exception as e:
                print(f"Error reading checkpoint file: {e}")
            return True, ids
        print("No checkpoint file found")
        return False, []

    def is_host_reachable(host_url, timeout=5, logger=None):
        try:
            if logger:
                print(f"Checking host reachability: {host_url}")
            response = httpx.get(host_url, timeout=timeout)
            if logger:
                print(f"Host response status: {response.status_code}")
            return response.status_code == 200
        except Exception as e:
            if logger:
                print(f"Error reaching host: {e}")
            return False

    async def main():
        # Validate langfuse URL string syntactically (quick check)
        print(f"Checking Langfuse host URL: {marimo_langfuse_host.value}")
        if not is_host_reachable(marimo_langfuse_host.value):
            print(f"{marimo_langfuse_host.value} Langfuse host URL could not be validated.")
            mo.output.replace(mo.md("❌ <b>Invalid Langfuse URL</b>: Please enter a valid URL."))
            return
        else:
            print("Langfuse URL reachable!")

        # Check Dify Agent API connectivity
        mo.status.spinner(title="Checking Dify Agent API Connectivity...")
        print("Starting Dify agent connectivity check with URL:", marimo_dify_base_url.value)
        ok, reason = check_dify_agent_connectivity(
            marimo_dify_base_url.value,
            marimo_dify_agent_key.value,
            logger
        )
        print(f"Dify agent connectivity result: ok={ok}, reason={reason}")
        if not ok:
            print(f"Could not verify Dify agent API key. Reason: {reason}")
            mo.output.replace(mo.md(f"❌ <b>Could not connect to Dify:</b> {reason}"))
            return
        else:
            print("Dify Agent connectivity validated.")

        # Check Dify Judge API
        mo.status.spinner(title="Checking Dify Judge API Connectivity...")
        print("Starting Dify judge connectivity check with URL:", marimo_dify_base_url.value)
        ok, reason = check_dify_judge_connectivity(
            marimo_dify_base_url.value,
            marimo_dify_judge_key.value,
            logger
        )
        print(f"Dify judge connectivity result: ok={ok}, reason={reason}")
        if not ok:
            print(f"Could not verify Dify judge API key. Reason: {reason}")
            mo.output.replace(mo.md(f"❌ <b>Could not connect to Dify:</b> {reason}"))
            return
        else:
            print("Dify Judge URL validated.")

        # Check Langfuse credentials
        mo.status.spinner("Checking Langfuse Credentials...")
        print(f"Checking Langfuse credentials for host: {marimo_langfuse_host.value}")
        ok, reason = check_langfuse_connectivity(
            marimo_lf_public_key.value,
            marimo_lf_secret_key.value,
            marimo_langfuse_host.value,
            logger)
        print(f"Langfuse connectivity: ok={ok}, reason={reason}")
        if not ok:
            print(f"Could not connect to langfuse. Reason: {reason}")
            mo.output.replace(mo.md(f"❌ <b>Could not connect to Langfuse:</b> {reason}"))
            return
        else:
            print("Langfuse credentials validated.")

        # Check dataset existence
        print("Checking dataset existence for:", marimo_dataset_name.value)
        lf_conn = Langfuse(
            public_key=marimo_lf_public_key.value,
            secret_key=marimo_lf_secret_key.value,
            host=marimo_langfuse_host.value
        )
        exists = check_dataset_exists(lf_conn, marimo_dataset_name.value, logger)
        print(f"Dataset exists: {exists}")
        if not exists:
            print(f"Could not locate the langfuse dataset: {marimo_dataset_name.value}")
            mo.output.replace(mo.md(f"❌ <b>Dataset not found</b>: Could not locate dataset '{marimo_dataset_name.value}'."))
            return

        # Check experiment name state
        expt_name_clean = marimo_experiment_name.value.strip().lower().replace(' ', '_')
        print(f"Cleaned experiment name: {expt_name_clean}")
        expt_exists, completed_ids = existing_experiment_checkpoint(marimo_dataset_name.value, expt_name_clean, logger)
        print(f"Experiment checkpoint exists: {expt_exists}, completed ids: {completed_ids}")

        if expt_exists:
            # Fetch dataset items
            try:
                ds_items = lf_conn.get_dataset(marimo_dataset_name.value).items
                print(f"Dataset items length: {len(ds_items)}")
            except Exception as e:
                print(f"Error fetching dataset items: {e}")
                ds_items = []

            ds_ids = set()
            for i, item in enumerate(ds_items):
                try:
                    item_id = getattr(item, "id", None) or f"idx_{i}"
                    ds_ids.add(item_id)
                except Exception as e:
                    print(f"Error processing item at index {i}: {e}")
            print(f"Dataset IDs: {ds_ids}")
            completed_set = set(completed_ids)
            print(f"Completed IDs: {completed_set}")
            if ds_ids and completed_set.issuperset(ds_ids):
                print(f"Evaluation '{marimo_experiment_name.value}' already completed all items.")
                mo.output.replace(mo.md(
                    f"❌ <b>An evaluation with experiment name '{marimo_experiment_name.value}' has already completed all items.</b> Please choose a different experiment name."
                ))
                return

        # Create evaluator and process
        try:
            print("Creating Evaluation object with parameters:")
            print(f"  experiment_name={marimo_experiment_name.value}")
            print(f"  langfuse_public_key={marimo_lf_public_key.value}")
            print(f"  langfuse_secret_key={marimo_lf_secret_key.value}")
            print(f"  langfuse_host={marimo_langfuse_host.value}")
            print(f"  dataset_name={marimo_dataset_name.value}")
            print(f"  dify_agent_key={marimo_dify_agent_key.value}")
            print(f"  dify_judge_key={marimo_dify_judge_key.value}")
            print(f"  dify_base_url={marimo_dify_base_url.value}")
            print(f"  instructions_inputs={marimo_judge_inputs.value}")
            print(f"  instructions_objective={marimo_judge_objective.value}")
            print(f"  instructions_guidelines={marimo_judge_guidelines.value}")
            print(f"  instructions_fewshot={marimo_shot_examples.value}")
            evaluator = Evaluation(
                experiment_name=marimo_experiment_name.value,
                langfuse_public_key=marimo_lf_public_key.value,
                langfuse_secret_key=marimo_lf_secret_key.value,
                langfuse_host=marimo_langfuse_host.value,
                dataset_name=marimo_dataset_name.value,
                dify_agent_key=marimo_dify_agent_key.value,
                dify_judge_key=marimo_dify_judge_key.value,
                dify_base_url=marimo_dify_base_url.value,
                dify_workflow_url=f"{marimo_dify_base_url.value.rstrip('/')}/workflows/run",
                instructions_inputs=marimo_judge_inputs.value,
                instructions_objective=marimo_judge_objective.value,
                instructions_guidelines=marimo_judge_guidelines.value,
                instructions_fewshot=marimo_shot_examples.value,
            )
            print("Calling evaluator.process()")
            await evaluator.process()
            print("Evaluation process completed")
            print("Finished evaluation. Check Langfuse for results.")
            mo.replace(mo.md(f"✅ Finished. Check Langfuse for results."))
        except Exception as e:
            print(f"Error during evaluation: {e}")
            print(f"Error during evaluation: {e}")
            mo.md(f"❌ Error occurred: {str(e)}")

    # Start the asynchronous main process
    await main()
    return


if __name__ == "__main__":
    app.run()
